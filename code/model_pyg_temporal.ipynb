{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ac905750",
   "metadata": {},
   "source": [
    "# PyTorch Geometric Temporal Model (GConvGRU)\n",
    "\n",
    "This notebook trains a temporal graph neural network using GConvGRU for engine fault classification."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5169d109",
   "metadata": {},
   "source": [
    "## 1. Setup and Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e919e8e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported successfully.\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import time\n",
    "from pathlib import Path\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric_temporal.nn.recurrent import GConvGRU\n",
    "from torch_geometric.nn import global_mean_pool\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report, confusion_matrix\n",
    "from tqdm.auto import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set random seeds\n",
    "torch.manual_seed(42)\n",
    "\n",
    "print(\"Libraries imported successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "82e60d99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train samples: 171443\n",
      "Val samples: 37658\n",
      "Test samples: 37658\n",
      "\n",
      "Sample structure:\n",
      "  x shape: torch.Size([20, 1, 10])  # [num_nodes, 1, window_size]\n",
      "  edge_index shape: torch.Size([2, 112])  # [2, num_edges]\n",
      "  y: 1  (dtype: torch.int64)\n"
     ]
    }
   ],
   "source": [
    "# Load datasets\n",
    "data_dir = Path(\"../results/datasets\")\n",
    "\n",
    "train_data = torch.load(data_dir / \"train_pyg_temporal.pt\", weights_only=False)\n",
    "val_data = torch.load(data_dir / \"val_pyg_temporal.pt\", weights_only=False)\n",
    "test_data = torch.load(data_dir / \"test_pyg_temporal.pt\", weights_only=False)\n",
    "\n",
    "print(f\"Train samples: {len(train_data['samples'])}\")\n",
    "print(f\"Val samples: {len(val_data['samples'])}\")\n",
    "print(f\"Test samples: {len(test_data['samples'])}\")\n",
    "\n",
    "# Inspect sample\n",
    "sample = train_data['samples'][0]\n",
    "print(f\"\\nSample structure:\")\n",
    "print(f\"  x shape: {sample.x.shape}  # [num_nodes, 1, window_size]\")\n",
    "print(f\"  edge_index shape: {sample.edge_index.shape}  # [2, num_edges]\")\n",
    "print(f\"  y: {sample.y}  (dtype: {sample.y.dtype})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "62863802",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration:\n",
      "  Batch size: 8192\n",
      "  Num nodes: 20\n",
      "  Num features: 1\n",
      "  Window size: 10\n",
      "  Hidden dim: 64\n",
      "  Num classes: 5\n"
     ]
    }
   ],
   "source": [
    "# Configuration\n",
    "BATCH_SIZE = 2048 * 4\n",
    "NUM_NODES = sample.x.shape[0]  # 20 sensors\n",
    "NUM_FEATURES = sample.x.shape[1]  # 1 feature per node\n",
    "WINDOW_SIZE = sample.x.shape[2]  # 10 timesteps\n",
    "NUM_CLASSES = 5  # 5 fault types\n",
    "HIDDEN_DIM = 64\n",
    "NUM_EPOCHS = 50\n",
    "LEARNING_RATE = 0.0001\n",
    "\n",
    "print(f\"Configuration:\")\n",
    "print(f\"  Batch size: {BATCH_SIZE}\")\n",
    "print(f\"  Num nodes: {NUM_NODES}\")\n",
    "print(f\"  Num features: {NUM_FEATURES}\")\n",
    "print(f\"  Window size: {WINDOW_SIZE}\")\n",
    "print(f\"  Hidden dim: {HIDDEN_DIM}\")\n",
    "print(f\"  Num classes: {NUM_CLASSES}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d61a9bdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataLoaders created:\n",
      "  Train batches: 21\n",
      "  Val batches: 5\n",
      "  Test batches: 5\n"
     ]
    }
   ],
   "source": [
    "# Create DataLoaders\n",
    "train_loader = DataLoader(train_data['samples'], batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_data['samples'], batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_loader = DataLoader(test_data['samples'], batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "print(f\"DataLoaders created:\")\n",
    "print(f\"  Train batches: {len(train_loader)}\")\n",
    "print(f\"  Val batches: {len(val_loader)}\")\n",
    "print(f\"  Test batches: {len(test_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3145d44b",
   "metadata": {},
   "source": [
    "## 2. Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3582a18f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model class defined.\n"
     ]
    }
   ],
   "source": [
    "class TemporalGNN(nn.Module):\n",
    "    def __init__(self, num_features, hidden_dim=64, num_classes=5):\n",
    "        super(TemporalGNN, self).__init__()\n",
    "        \n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        # Temporal layer\n",
    "        self.temporal = GConvGRU(\n",
    "            in_channels=num_features,\n",
    "            out_channels=hidden_dim,\n",
    "            K=2\n",
    "        )\n",
    "        \n",
    "        # Classification head\n",
    "        self.fc = nn.Linear(hidden_dim, num_classes)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        \n",
    "    def forward(self, data):\n",
    "        x, edge_index, batch = data.x, data.edge_index, data.batch\n",
    "        time_steps = x.size(2)\n",
    "        \n",
    "        # Process temporal dimension\n",
    "        h = None\n",
    "        for t in range(time_steps):\n",
    "            x_t = x[:, :, t]  # [total_nodes, num_features]\n",
    "            h = self.temporal(x_t, edge_index, H=h)\n",
    "        \n",
    "        # Pool each graph separately\n",
    "        h_graph = global_mean_pool(h, batch)  # [batch_size, hidden_dim]\n",
    "        \n",
    "        # Classification\n",
    "        h_graph = self.dropout(h_graph)\n",
    "        logits = self.fc(h_graph)\n",
    "        \n",
    "        return logits\n",
    "\n",
    "print(\"Model class defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ece2d0ea",
   "metadata": {},
   "source": [
    "## 3. Training and Evaluation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f60abe11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training and evaluation functions defined.\n"
     ]
    }
   ],
   "source": [
    "def train_epoch(model, loader, criterion, optimizer, device):\n",
    "    \"\"\"Train for one epoch.\"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    pbar = tqdm(loader, desc='Training', leave=False)\n",
    "    for batch in pbar:\n",
    "        batch = batch.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        logits = model(batch)\n",
    "        loss = criterion(logits, batch.y)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        preds = logits.argmax(dim=1).cpu().numpy()\n",
    "        all_preds.extend(preds)\n",
    "        all_labels.extend(batch.y.cpu().numpy())\n",
    "        \n",
    "        pbar.set_postfix({'loss': f'{loss.item():.4f}'})\n",
    "    \n",
    "    avg_loss = total_loss / len(loader)\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    f1 = f1_score(all_labels, all_preds, average='weighted')\n",
    "    \n",
    "    return avg_loss, accuracy, f1\n",
    "\n",
    "\n",
    "def evaluate(model, loader, criterion, device):\n",
    "    \"\"\"Evaluate the model.\"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(loader, desc='Evaluating', leave=False):\n",
    "            batch = batch.to(device)\n",
    "            \n",
    "            logits = model(batch)\n",
    "            loss = criterion(logits, batch.y)\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            preds = logits.argmax(dim=1).cpu().numpy()\n",
    "            all_preds.extend(preds)\n",
    "            all_labels.extend(batch.y.cpu().numpy())\n",
    "    \n",
    "    avg_loss = total_loss / len(loader)\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    f1 = f1_score(all_labels, all_preds, average='weighted')\n",
    "    \n",
    "    return avg_loss, accuracy, f1, all_preds, all_labels\n",
    "\n",
    "print(\"Training and evaluation functions defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a3a8732",
   "metadata": {},
   "source": [
    "## 4. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c7290425",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "Model parameters: 25,669\n"
     ]
    }
   ],
   "source": [
    "# Initialize model\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = TemporalGNN(\n",
    "    num_features=NUM_FEATURES,\n",
    "    hidden_dim=HIDDEN_DIM,\n",
    "    num_classes=NUM_CLASSES\n",
    ").to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=1e-5)\n",
    "\n",
    "print(f\"Device: {device}\")\n",
    "print(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7feac577",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n",
      "======================================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4bc5e0cd688748c7b170cf61cee3fe45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/21 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f7391ba49c54bc9b864edc895b48da9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/50\n",
      "  Train Loss: 1.6617 | Acc: 0.1599 | F1: 0.1074\n",
      "  Val   Loss: 1.6145 | Acc: 0.2235 | F1: 0.0816\n",
      "  Epoch time: 34.0s\n",
      "  ✓ Best model saved (F1: 0.0816)\n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "970b7386e84446d18030c2e47ccf6a6d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/21 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3610b78c8b84ed9af622184082e0ffb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2/50\n",
      "  Train Loss: 1.6443 | Acc: 0.1717 | F1: 0.1335\n",
      "  Val   Loss: 1.6011 | Acc: 0.2235 | F1: 0.0816\n",
      "  Epoch time: 36.7s\n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7688802140de4d888f649011743c1a03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/21 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d9a6e8c8d404ffcb467877dafc3fa5e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 3/50\n",
      "  Train Loss: 1.6268 | Acc: 0.1888 | F1: 0.1637\n",
      "  Val   Loss: 1.5848 | Acc: 0.2235 | F1: 0.0816\n",
      "  Epoch time: 34.4s\n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c48e1ada64a47e1a1db4ea0fcaac30e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/21 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70b9eb20eda848c49205da6fe82e8636",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 4/50\n",
      "  Train Loss: 1.6107 | Acc: 0.2143 | F1: 0.1944\n",
      "  Val   Loss: 1.5746 | Acc: 0.2235 | F1: 0.0816\n",
      "  Epoch time: 37.0s\n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "359f18d34b054a0294218f094ee94e33",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/21 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7c4c740829142cd896b1e5af1aefaa6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 5/50\n",
      "  Train Loss: 1.5981 | Acc: 0.2452 | F1: 0.2181\n",
      "  Val   Loss: 1.5651 | Acc: 0.3535 | F1: 0.1846\n",
      "  Epoch time: 34.9s\n",
      "  ✓ Best model saved (F1: 0.1846)\n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2ea46c7f7eb444a908fb9a69d8a82a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/21 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "182606cda33c4b3bbce5d9ef88521878",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 6/50\n",
      "  Train Loss: 1.5870 | Acc: 0.2750 | F1: 0.2317\n",
      "  Val   Loss: 1.5566 | Acc: 0.3535 | F1: 0.1846\n",
      "  Epoch time: 37.5s\n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "295e862f8a92449abed031e089f7c7bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/21 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad6bd81523c74b94b28dd193b0eaa487",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 7/50\n",
      "  Train Loss: 1.5769 | Acc: 0.3034 | F1: 0.2376\n",
      "  Val   Loss: 1.5492 | Acc: 0.3535 | F1: 0.1846\n",
      "  Epoch time: 37.5s\n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4cbb15730d884817a984df58a3bc8e18",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/21 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "600d94f2c9ae42d187492efe2638afa5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 8/50\n",
      "  Train Loss: 1.5674 | Acc: 0.3280 | F1: 0.2390\n",
      "  Val   Loss: 1.5431 | Acc: 0.3535 | F1: 0.1846\n",
      "  Epoch time: 35.0s\n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d9ed2373cba41e58a34aec584151308",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/21 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33e956e9082e4828b3a0f957f5183cf7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 9/50\n",
      "  Train Loss: 1.5606 | Acc: 0.3443 | F1: 0.2354\n",
      "  Val   Loss: 1.5383 | Acc: 0.3535 | F1: 0.1846\n",
      "  Epoch time: 37.1s\n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "103719a62ce7495abe8bda3b4e417411",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/21 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45f0085211dc4b6eb3b69d7a17cb136f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 10/50\n",
      "  Train Loss: 1.5534 | Acc: 0.3581 | F1: 0.2317\n",
      "  Val   Loss: 1.5348 | Acc: 0.3535 | F1: 0.1846\n",
      "  Epoch time: 34.6s\n",
      "\n",
      "Early stopping at epoch 10\n",
      "\n",
      "Training completed in 5.98 minutes\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "best_val_f1 = 0\n",
    "patience = 5\n",
    "patience_counter = 0\n",
    "\n",
    "history = {\n",
    "    'train_loss': [], 'train_acc': [], 'train_f1': [],\n",
    "    'val_loss': [], 'val_acc': [], 'val_f1': []\n",
    "}\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "print(\"Starting training...\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    epoch_start = time.time()\n",
    "    \n",
    "    # Train\n",
    "    train_loss, train_acc, train_f1 = train_epoch(model, train_loader, criterion, optimizer, device)\n",
    "    \n",
    "    # Validate\n",
    "    val_loss, val_acc, val_f1, _, _ = evaluate(model, val_loader, criterion, device)\n",
    "    \n",
    "    epoch_time = time.time() - epoch_start\n",
    "    \n",
    "    # Store history\n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['train_acc'].append(train_acc)\n",
    "    history['train_f1'].append(train_f1)\n",
    "    history['val_loss'].append(val_loss)\n",
    "    history['val_acc'].append(val_acc)\n",
    "    history['val_f1'].append(val_f1)\n",
    "    \n",
    "    # Print progress\n",
    "    print(f\"\\nEpoch {epoch+1}/{NUM_EPOCHS}\")\n",
    "    print(f\"  Train Loss: {train_loss:.4f} | Acc: {train_acc:.4f} | F1: {train_f1:.4f}\")\n",
    "    print(f\"  Val   Loss: {val_loss:.4f} | Acc: {val_acc:.4f} | F1: {val_f1:.4f}\")\n",
    "    print(f\"  Epoch time: {epoch_time:.1f}s\")\n",
    "    \n",
    "    # Early stopping\n",
    "    if val_f1 > best_val_f1:\n",
    "        best_val_f1 = val_f1\n",
    "        patience_counter = 0\n",
    "        torch.save(model.state_dict(), \"../results/best_pyg_temporal_model.pt\")\n",
    "        print(f\"  ✓ Best model saved (F1: {best_val_f1:.4f})\")\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= patience:\n",
    "            print(f\"\\nEarly stopping at epoch {epoch+1}\")\n",
    "            break\n",
    "    \n",
    "    print(\"-\" * 70)\n",
    "\n",
    "elapsed_time = time.time() - start_time\n",
    "print(f\"\\nTraining completed in {elapsed_time/60:.2f} minutes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a728125",
   "metadata": {},
   "source": [
    "## 5. Evaluation on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9292cdb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best model\n",
    "model.load_state_dict(torch.load(\"../results/best_pyg_temporal_model.pt\"))\n",
    "\n",
    "# Evaluate on test set\n",
    "test_loss, test_acc, test_f1, test_preds, test_labels = evaluate(model, test_loader, criterion, device)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"TEST SET RESULTS\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Test Loss: {test_loss:.4f}\")\n",
    "print(f\"Test Accuracy: {test_acc:.4f}\")\n",
    "print(f\"Test F1 Score: {test_f1:.4f}\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ae6e440",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification report\n",
    "label_names = ['corrosion', 'erosion', 'fouling', 'tip_clearance', 'no_fault']\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(test_labels, test_preds, target_names=label_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70b641a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix\n",
    "cm = confusion_matrix(test_labels, test_preds)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=label_names, yticklabels=label_names)\n",
    "plt.title('Confusion Matrix - PyG Temporal Model')\n",
    "plt.ylabel('True Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.tight_layout()\n",
    "plt.savefig('../results/confusion_matrix_pyg_temporal.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c496057",
   "metadata": {},
   "source": [
    "## 6. Training History Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9852dadc",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# Loss\n",
    "axes[0].plot(history['train_loss'], label='Train')\n",
    "axes[0].plot(history['val_loss'], label='Validation')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].set_title('Loss History')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Accuracy\n",
    "axes[1].plot(history['train_acc'], label='Train')\n",
    "axes[1].plot(history['val_acc'], label='Validation')\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Accuracy')\n",
    "axes[1].set_title('Accuracy History')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "# F1 Score\n",
    "axes[2].plot(history['train_f1'], label='Train')\n",
    "axes[2].plot(history['val_f1'], label='Validation')\n",
    "axes[2].set_xlabel('Epoch')\n",
    "axes[2].set_ylabel('F1 Score')\n",
    "axes[2].set_title('F1 Score History')\n",
    "axes[2].legend()\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../results/training_history_pyg_temporal.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.12.4)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
